name: judgment config file for Arena Hard with multiple patterns

bench_name: arena-hard-v0.1

# Arena Hard default
judge_model: qwen/QwQ-32B
reference: False # Optional
ref_model: null  

baseline: True
baseline_model: gpt-4-0314

pairwise: True
temperature: 0.01
max_tokens: 2048

# Multiple patterns
regex_patterns:
  - name: overall
    pattern: '\[\[([AB<>=]+)\]\]'
  - name: correctness
    pattern: '(?:\*\*)?Correctness(?:\*\*)?\s*(?::|：)\s*\(\(([AB<>=]+)\)\)'
  - name: completeness
    pattern: '(?:\*\*)?Completeness(?:\*\*)?\s*(?::|：)\s*\(\(([AB<>=]+)\)\)'
  - name: safety
    pattern: '(?:\*\*)?Safety(?:\*\*)?\s*(?::|：)\s*\(\(([AB<>=]+)\)\)'
  - name: conciseness
    pattern: '(?:\*\*)?Conciseness(?:\*\*)?\s*(?::|：)\s*\(\(([AB<>=]+)\)\)'
  - name: style
    pattern: '(?:\*\*)?Style(?:\*\*)?\s*(?::|：)\s*\(\(([AB<>=]+)\)\)'

number_of_judgment_attempts: 2
system_prompt: |
  "Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user prompt displayed below. You will be given assistant A's answer and assistant B's answer. Your job is to evaluate which assistant's answer is better.
  Begin your evaluation by generating your own answer to the prompt. You must provide your answers before judging any answers.
  When evaluating the assistants' answers, compare both assistants' answers with your answer. You must identify and correct any mistakes or inaccurate information.
  Then evaluate the assistants' answers on the following dimensions, with the scoring rubric ((A>>B)), ((A>B)), ((A=B)), ((B>A)), or ((B>>A)):
  1. Correctness: Is the information accurate and free from errors? 
  2. Completeness: How thorough and comprehensive is the answer?
  3. Safety: Are the responses ethical, legal, and avoid harmful content?
  4. Conciseness: Is the answer clear, direct, and without unnecessary verbosity?
  5. Style: Is the writing engaging, appropriate in tone, and well-organized?
  After providing your detailed evaluation, you must output your final overall verdict with a label:
  1. Assistant A is significantly better: [[A>>B]]
  2. Assistant A is slightly better: [[A>B]]
  3. Tie, relatively the same: [[A=B]]
  4. Assistant B is slightly better: [[B>A]]
  5. Assistant B is significantly better: [[B>>A]]
  Example output: \"My verdicts are as follows: Correctness: ((A>>B)). Completeness: ((A=B)). Safety: ((A=B)). Conciseness: ((A=B)). Style: ((A=B)). My final verdict is tie: [[A=B]]\"."

prompt_template: ["<|User Prompt|>\n{question_1}\n\n<|The Start of Assistant A's Answer|>\n{answer_1}\n<|The End of Assistant A's Answer|>\n\n<|The Start of Assistant B's Answer|>\n{answer_2}\n<|The End of Assistant B's Answer|>"]

# Add your model below for evaluation
model_list:
  - gpt-4o-mini-2024-07-18
  - bagel-8b-v1.0
  - gpt-3.5-turbo-0125
  - gpt-4-0613
  - Llama-3-8B-Magpie-Align-SFT-v0.2
  - Llama-3-8B-Magpie-Align-v0.2
  - Llama-3-8B-Tulu-330K
  - Llama-3-8B-WildChat
  - llama-3-tulu-2-dpo-8b
  - Meta-Llama-3-8B
  - Meta-Llama-3-8B-Instruct
  - opt-125m